general:
  seed: 42 # ensuring reproducibility
data_loader:
  path: creditcard.csv
train_test_split:
  test_size: 0.05
  train_sizes: [0.5]
  
optimization:
  cross_validator: 5
  direction: maximize
  metric: f1
  n_trials: 10

  param_grid:
    XGBClassifier:
      fixed:
        booster: "gbtree"
        objective: "binary:logistic"
        eval_metric: "error"
        random_state: 42
        verbosity: 1
      tunable:
        n_estimators: [2, 100]
        max_depth: [3, 20]
        learning_rate: [0.001, 0.5]
        gamma: [0, 10]
        subsample: [0.5, 1]
      float_params: ["learning_rate", "gamma", "subsample"]
      categ_params: []
    LGBMClassifier:
      fixed:
        boosting_type: "gbdt"
        subsample_for_bin: 200000
        objective: "binary"
        random_state: 42
        verbose: 0
      tunable:
        num_leaves: [5, 200]
        max_depth: [5, 20]
        learning_rate : [0.01, 0.5]
        n_estimators: [50, 200]
        colsample_bytree: [0.3, 1.0]
        reg_alpha: [0.1, 10.0]
        reg_lambda: [0.1, 10.0]
      float_params: ["learning_rate", "colsample_bytree", "reg_alpha", "reg_lambda"]
      categ_params: []
    GradientBoostingClassifier:
      fixed:
        loss: "log_loss"
        random_state: 42
        verbose: 3
      tunable:
        learning_rate: [0.001, 0.5]
        n_estimators: [2, 100]
        subsample: [0.5, 1]
        min_samples_split: [2, 10]
        max_depth: [3, 20]
        max_features: [5, 30]
      float_params: ["learning_rate", "subsample"]
      categ_params: []
    RandomForestClassifier:
      fixed:
        criterion: "gini"
        bootstrap: True
        oob_score: False
        verbose: 0
        random_state: 42
      tunable:
        n_estimators: [3, 100]
        max_depth: [2, 20]
        min_samples_split: [2, 10]
        min_samples_leaf: [1, 10]
        max_features: [5, 30]
      float_params: []
      categ_params: []
    DecisionTreeClassifier:
      fixed:
        criterion: "gini"
        splitter: "best"
        random_state: 42
      tunable:
        max_depth: [3, 20]
        min_samples_leaf: [2, 20]
      float_params: []
      categ_params: []
    LogisticRegression:
      fixed:
        penalty: "l2"
        dual: False
        fit_intercept: True
        random_state: 42
        solver: "lbfgs"
        #multi_class: "ovr"
        verbose: 0
      tunable:
        tol: [0.00001, 2]
        C: [0.2, 5]
        max_iter: [100, 1000]
      float_params: ["tol", "C"]
      categ_params: []
    KNeighborsClassifier:
      fixed:
        weights: "uniform"
        # algorithm: "auto" <- causing error
        # random_state: 42
      tunable:
        n_neighbors: [5, 100]
        leaf_size: [15, 100]
        p: [2, 10]
      float_params: []
      categ_params: []
    MLPClassifier:
      fixed:
        activation: "logistic"
        solver: "adam"
        learning_rate: "constant"
        shuffle: True
        random_state: 42
        tol: 0.0001
        n_iter_no_change: 5
        max_fun: 10000
        max_iter: 100
      tunable:
        hidden_layer_sizes:
          n_layers: [1, 3]
          n_units: [50, 100]
        alpha: [0.001, 0.1]
        batch_size: [50, 200]
        learning_rate_init: [0.001, 0.1]
        beta_1: [0.0, 1.0]
        beta_2: [0.0, 1.0]
      float_params: ["alpha", "learning_rate_init", "beta_1", "beta_2"]
      categ_params: []
    NeuralNetworkClassifier:
      fixed:
        input_size: 30
        loss: "binary_crossentropy"
        scoring_metric: "accuracy"
        epochs: 5
        batch_size: 64
      tunable:
        n_layers: [8, 16]
        n_units: [32, 256]
        activation_function: ["sigmoid", "relu"]
        learning_rate: [0.0001, 0.01]
      float_params: ["learning_rate"]
      categ_params: ["activation_function"]

  feature_selection:
    tolerance: 0.0005
